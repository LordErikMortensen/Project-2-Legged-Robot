{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LHb4gM3uFfnF","outputId":"e56e3558-b035-4133-f0ff-1c880ade595d"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Pablo\\Desktop\\Code_vic\\legged_robots\\Project-2-Legged-Robot\\.conda\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n","c:\\Users\\Pablo\\Desktop\\Code_vic\\legged_robots\\Project-2-Legged-Robot\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Using cpu device\n","=================================== Save path is ./logs/intermediate_models/121723205127/\n","=================================== Save path is ./logs/intermediate_models/121723205127/\n","=================================== Save path is ./logs/intermediate_models/121723205127/\n","=================================== Save path is ./logs/intermediate_models/121723205127/\n","=================================== Save path is ./logs/intermediate_models/121723205127/\n","=================================== Save path is ./logs/intermediate_models/121723205127/\n","=================================== Save path is ./logs/intermediate_models/121723205127/\n","=================================== Save path is ./logs/intermediate_models/121723205127/\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 761      |\n","|    ep_rew_mean     | 3.38e+13 |\n","| time/              |          |\n","|    fps             | 134      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 30       |\n","|    total_timesteps | 4096     |\n","---------------------------------\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["\"\"\"\n","Run stable baselines 3 on quadruped env \n","Check the documentation! https://stable-baselines3.readthedocs.io/en/master/\n","\"\"\"\n","%load_ext autoreload\n","%autoreload 2\n","\n","import os\n","from datetime import datetime\n","# stable baselines 3\n","from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n","from stable_baselines3 import PPO, SAC\n","from stable_baselines3.common.env_util import make_vec_env\n","# utils\n","from utils.utils import CheckpointCallback\n","from utils.file_utils import get_latest_model\n","# gym environment\n","from env.quadruped_gym_env import QuadrupedGymEnv\n","\n","\n","LEARNING_ALG = \"PPO\" # or \"SAC\"\n","LOAD_NN = False # if you want to initialize training with a previous model \n","NUM_ENVS = 1    # how many pybullet environments to create for data collection\n","USE_GPU = False # make sure to install all necessary drivers \n","\n","# LEARNING_ALG = \"SAC\";  USE_GPU = True\n","# after implementing, you will want to test how well the agent learns with your MDP: \n","# env_configs = {\"motor_control_mode\":\"CPG\", #CPG, PD, CARTESIAN_PD\n","#                \"task_env\": \"FLAGRUN\", #\"LR_COURSE_TASK\", \n","#                \"observation_space_mode\": \"LR_COURSE_OBS\"}\n","env_configs = {\"motor_control_mode\":\"PD\", #CPG, PD, CARTESIAN_PD\n","               \"task_env\": \"LR_COURSE_TASK\", #  \"LR_COURSE_TASK\", -> Controls the cost function\n","               \"observation_space_mode\": \"LR_COURSE_OBS\"} #Controls the observation space\n","\n","if USE_GPU and LEARNING_ALG==\"SAC\":\n","    gpu_arg = \"auto\" \n","else:\n","    gpu_arg = \"cpu\"\n","\n","if LOAD_NN:\n","    interm_dir = \"./logs/intermediate_models/\"\n","    log_dir = interm_dir + '' # add path\n","    stats_path = os.path.join(log_dir, \"vec_normalize.pkl\")\n","    model_name = get_latest_model(log_dir)\n","\n","# directory to save policies and normalization parameters\n","SAVE_PATH = './logs/intermediate_models/'+ datetime.now().strftime(\"%m%d%y%H%M%S\") + '/'\n","os.makedirs(SAVE_PATH, exist_ok=True)\n","# checkpoint to save policy network periodically\n","checkpoint_callback = CheckpointCallback(save_freq=30000, save_path=SAVE_PATH,name_prefix='rl_model', verbose=2)\n","# create Vectorized gym environment\n","env = lambda: QuadrupedGymEnv(**env_configs)  \n","env = make_vec_env(env, monitor_dir=SAVE_PATH,n_envs=NUM_ENVS)\n","# normalize observations to stabilize learning (why?)\n","env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=100.)\n","\n","if LOAD_NN:\n","    env = lambda: QuadrupedGymEnv()\n","    env = make_vec_env(env, n_envs=NUM_ENVS)\n","    env = VecNormalize.load(stats_path, env)\n","\n","# Multi-layer perceptron (MLP) policy of two layers of size _,_ \n","policy_kwargs = dict(net_arch=[256,256])\n","# What are these hyperparameters? Check here: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n","n_steps = 4096 \n","learning_rate = lambda f: 1e-4 \n","ppo_config = {  \"gamma\":0.99, \n","                \"n_steps\": int(n_steps/NUM_ENVS), \n","                \"ent_coef\":0.0, \n","                \"learning_rate\":learning_rate, \n","                \"vf_coef\":0.5,\n","                \"max_grad_norm\":0.5, \n","                \"gae_lambda\":0.95, \n","                \"batch_size\":128,\n","                \"n_epochs\":10, \n","                \"clip_range\":0.2, \n","                \"clip_range_vf\":1,\n","                \"verbose\":1, \n","                \"tensorboard_log\":None, \n","                \"_init_setup_model\":True, \n","                \"policy_kwargs\":policy_kwargs,\n","                \"device\": gpu_arg}\n","\n","# What are these hyperparameters? Check here: https://stable-baselines3.readthedocs.io/en/master/modules/sac.html\n","sac_config={\"learning_rate\":1e-4,\n","            \"buffer_size\":300000,\n","            \"batch_size\":256,\n","            \"ent_coef\":'auto', \n","            \"gamma\":0.99, \n","            \"tau\":0.005,\n","            \"train_freq\":1, \n","            \"gradient_steps\":1,\n","            \"learning_starts\": 10000,\n","            \"verbose\":1, \n","            \"tensorboard_log\":None,\n","            \"policy_kwargs\": policy_kwargs,\n","            \"seed\":None, \n","            \"device\": gpu_arg}\n","\n","if LEARNING_ALG == \"PPO\":\n","    model = PPO('MlpPolicy', env, **ppo_config)\n","elif LEARNING_ALG == \"SAC\":\n","    model = SAC('MlpPolicy', env, **sac_config)\n","else:\n","    raise ValueError(LEARNING_ALG + 'not implemented')\n","\n","if LOAD_NN:\n","    if LEARNING_ALG == \"PPO\":\n","        model = PPO.load(model_name, env)\n","    elif LEARNING_ALG == \"SAC\":\n","        model = SAC.load(model_name, env)\n","    print(\"\\nLoaded model\", model_name, \"\\n\")\n","\n","# Learn and save (may need to train for longer)\n","model.learn(total_timesteps=1000000, log_interval=1,callback=checkpoint_callback)\n","# Don't forget to save the VecNormalize statistics when saving the agent\n","model.save( os.path.join(SAVE_PATH, \"rl_model\" ) ) \n","env.save(os.path.join(SAVE_PATH, \"vec_normalize.pkl\" )) \n","if LEARNING_ALG == \"SAC\": # save replay buffer \n","    model.save_replay_buffer(os.path.join(SAVE_PATH,\"off_policy_replay_buffer\"))\n","\n","### CURRENTLY RUNNING env_configs = {\"motor_control_mode\":\"CARTESIAN_PD\", #CPG, PD, CARTESIAN_PD\n","            #    \"task_env\": \"LR_COURSE_TASK\", #  \"LR_COURSE_TASK\", -> Controls the cost function\n","            #    \"observation_space_mode\": \"LR_COURSE_OBS\"} #Controls the observation space\n","            # SLIGHT MODIFS TO COST FUNCTION, joint motion miore penalized"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For ./logs/intermediate_models/121623233019/\n","# def _reward_lr_course(self):\n","#       \"\"\" Implement your reward function here. How will you improve upon the above? \"\"\"\n","#       # [TODO] add your reward function. -> based on slides of lect7 \n","#       # source: \"Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning\"\n","#       dt = self._time_step\n","#       desired_base_height = 0.3 #correct height?\n","#       des_vel_x = 0.5\n","#       des_vel_y = 0\n","#       des_ang_vel_z = 0\n","#       curr_dist_to_goal, angle = self.get_distance_and_angle_to_goal()\n","#       linear_vel = self.robot.GetBaseLinearVelocity()\n","#       ang_vel = self.robot.GetBaseAngularVelocity()\n","\n","#       # _reward_fwd_locomotion rewards linear velocity along x and penalizes yaw, drift and energy\n","#       reward_loco = self._reward_fwd_locomotion(des_vel_x)\n","      \n","#       # Rewards movements that brings the robot closer to the goal\n","#       dist_reward = 10 * (self._prev_pos_to_goal - curr_dist_to_goal)\n","      \n","#       # Linear velocity tracking (already done for x in reward_loco):\n","#       vely_tracking_reward = 1 * dt * np.exp(-1/0.25 * (linear_vel[1] - des_vel_y)**2)\n","      \n","#       # Linear velocity penality (in z):\n","#       velz_penalty = -4 * dt * linear_vel[2]**2\n","      \n","#       # Angluar velocity tracking (in z):\n","#       ang_velz_tracking_reward = 0.5 * dt * np.exp(-1/0.25 * (ang_vel[2] - des_ang_vel_z)*2)\n","      \n","#       # Peanlize wrong height in z (necessary??)\n","#       height_penalty = -0.1 * dt * np.abs(self.robot.GetBasePosition()[2] - desired_base_height)\n","      \n","#       # penalize angular velocities along x and y axis\n","#       ang_velxy_penalty = -0.1 * dt * (linear_vel[0]*2 + linear_vel[1]*2)\n","      \n","      \n","#       # penalize roll? Is this redundant with penalizing angluar velocities?\n","#       # roll_penalty = -some_factor * np.abs(self.robot.GetBaseOrientationRollPitchYaw()[0])\n","      \n","#       # penalize pitch? Is this redundant with penalizing angluar velocities?\n","#       # pitch_penalty = -some_factor * np.abs(self.robot.GetBaseOrientationRollPitchYaw()[1])\n","      \n","      \n","#       # Create a smoother, more natural motion (penalize joint motion and joint torques)\n","#       joint_motion = 0\n","#       joint_torques = 0\n","      \n","#       for tau,vel in zip(self._dt_motor_torques,self._dt_motor_velocities):\n","#           #joint_motion -= 0.001 * dt (np.norm(???)2 + np.norm(vel)*2) #How to get joint acceleration?\n","#           joint_torques -= 0.0001 * dt * np.linalg.norm(tau)**2\n","      \n","      \n","#       # Penalize collisions\n","#       _, col, _, _ = self.robot.GetContactInfo()\n","#       collisions = -0.001 * dt * col\n","        \n","      \n","#       # action_rate?\n","#       # feet_air_time? reward term to take longer steps -> more visually appealing behavior\n","        \n","#       tot_reward = reward_loco + dist_reward + vely_tracking_reward + velz_penalty + ang_velz_tracking_reward \\\n","#       + height_penalty + ang_velxy_penalty + joint_torques + collisions\n","      \n","#       return max(tot_reward, 0)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
